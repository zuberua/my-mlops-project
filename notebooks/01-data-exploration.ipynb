{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "This notebook explores the dataset used for model training.\n",
    "\n",
    "## Objectives\n",
    "- Load and inspect the data\n",
    "- Understand feature distributions\n",
    "- Identify data quality issues\n",
    "- Visualize relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from S3\n",
    "data_key = 'mlops-demo/input/data.csv'\n",
    "data_location = f's3://{bucket}/{data_key}'\n",
    "\n",
    "print(f\"Loading data from: {data_location}\")\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(data_location)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"Data Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming first column is target\n",
    "target_col = df.columns[0]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df[target_col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[target_col].value_counts().plot(kind='bar')\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features (excluding target)\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numeric_features:\n",
    "    numeric_features.remove(target_col)\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(numeric_features[:10])  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of first 9 features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_features[:9]):\n",
    "    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black')\n",
    "    axes[idx].set_title(f'{col}')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation = df[numeric_features[:20]].corr()  # First 20 features\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for features by target class\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_features[:6]):\n",
    "    df.boxplot(column=col, by=target_col, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{col} by {target_col}')\n",
    "    axes[idx].set_xlabel(target_col)\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Infinite values: {inf_count}\")\n",
    "\n",
    "# Check for constant features\n",
    "constant_features = [col for col in numeric_features if df[col].nunique() == 1]\n",
    "print(f\"\\nConstant features: {len(constant_features)}\")\n",
    "if constant_features:\n",
    "    print(constant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings\n",
    "\n",
    "### Summary\n",
    "- Dataset size: [rows] x [columns]\n",
    "- Target distribution: [balanced/imbalanced]\n",
    "- Missing values: [count]\n",
    "- Data quality issues: [list]\n",
    "\n",
    "### Recommendations\n",
    "1. [Recommendation 1]\n",
    "2. [Recommendation 2]\n",
    "3. [Recommendation 3]\n",
    "\n",
    "### Next Steps\n",
    "- Feature engineering (see notebook 02)\n",
    "- Model experimentation (see notebook 03)\n",
    "- Update preprocessing script with findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "summary = {\n",
    "    'dataset_shape': df.shape,\n",
    "    'missing_values': missing.to_dict(),\n",
    "    'target_distribution': df[target_col].value_counts().to_dict(),\n",
    "    'duplicates': int(duplicates),\n",
    "    'constant_features': constant_features\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('data_exploration_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Summary saved to data_exploration_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
